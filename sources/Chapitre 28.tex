\documentclass[10pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[french]{babel}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}
\usepackage{setspace}
\usepackage{ulem}
\usepackage{stmaryrd}
\usepackage{amsthm}
\usepackage{dsfont}
\usepackage{mathpazo}

\onehalfspacing

\theoremstyle{definition}
\newtheorem{proposition}{Proposition}[section]
\newtheorem{theorem}[proposition]{Théorème}
\newtheorem{corollaire}[proposition]{Corollaire}
\newtheorem{lemme}[proposition]{Lemme}
\newtheorem{definition}[proposition]{Définition}

\usepackage{array}
\newcolumntype{M}[1]{>{\centering\arraybackslash}m{#1}}

\DeclareMathOperator{\card}{Card}
\DeclareMathOperator{\im}{im}
\DeclareMathOperator{\cov}{cov}
\newcommand{\indep}{\mathrel{\perp \!\!\! \perp}}

\begin{document}
\renewcommand{\labelitemi}{$*$}
\begin{center}
{\Large \textbf{Chapitre 28: Probabilités}}
\end{center}

\section{Événements et variables aléatoires}
\subsection{Généralités}
\begin{definition}
\hfill
\begin{itemize}
\item Un \uline{univers} (fini) est un ensemble fini non vide $\Omega$
\item Un \uline{événement} est une partie $A \subseteq \Omega$
\item Une \uline{variable aléatoire} (VA) est une application $X: \Omega \to E$ vers un ensemble $E$
\end{itemize}
\end{definition}

\subsection{Opérations}
\noindent \uline{Image d'une VA par une application} \\
Étant donné une VA $X : \Omega \to E$ est une application $f: E \to F$, on définit la \uline{VA image de $X$ par $f$}, \\
$f(x)$ comme la composition $f \circ X: \Omega \to F$ \medskip

\noindent \uline{Événements définis par une VA} \\
Soit $X: \Omega \to E$ \\
Pour toute partie $S \subseteq E$, on définit l'événement
\[ (X \in S) = \left\{ X \in S \right\} = \left\{ \omega \in \Omega \mid X(\omega) \in S \right\} = X^{-1}[S] \] \medskip

\noindent \uline{Indicatrice d'un élément} \\
Tout événement $A \subseteq \Omega$ définit une VA
\[ \mathds{1}_A : \begin{cases}
\Omega \to \left\{ 0, 1 \right\} \\
\omega \mapsto \begin{cases}
1 \text{ si } \omega \in A \\
0 \text{ si } \omega \not\in A
\end{cases}
\end{cases} \]

\subsection{Expériences aléatoires}
\noindent Considérons un exemple d'expérience aléatoire. On joue à pile ou face $n$ fois de suite. \medskip

\noindent \uline{UNIVERS}: On considère $\Omega = \left\{ 0, 1 \right\}$ \\
Une \uline{issue} est un résultat possible, càd ici une suite de $n$ lancers. \medskip

\noindent \uline{ÉVÉNEMENTS}: L'événement (au sens usuel) "le $i$-ème lancer donne pile" correspond à l'événement \\
(au sens mathématique) $\pi_i = \left\{(b_1, ...\,, b_n) \in \Omega \mid b_i = 1 \right\}$ (ensemble à $2^{n - 1}$ événements)\\
"Obtenir que des 'face' " $F = \left\{ (0, 0, ...\,, 0) \right\}$ (est élémentaire = singleton) \\
"Obtenir un nombre impair de 'pile' " $I = \left\{ (b_1, ...\,, b_n) \in \Omega \mid b_1 + ... + b_n \equiv 1 (\text{mod } 2) \right\}$ \medskip

\noindent \uline{VARIABLES ALÉATOIRES}:
\begin{itemize}
\item $L_i : \begin{cases}
\Omega \to \{ 0, 1 \} \\
(b_1, ...\,, b_n) \mapsto b_i
\end{cases}$ "est le résultat du $i$-ème lancer"
\item $N : \begin{cases}
\Omega \to \llbracket 0, n \rrbracket \\
(b_1, ...\,, b_n) \mapsto b_1, ...\,, b_n
\end{cases}$ est le nombre de "pile" obtenus
\item $P: \begin{cases}
\Omega \to \llbracket 1, n \rrbracket \cup \{ +\infty \} \\
(b_1, ...\,, b_n) \mapsto \min\left\{ i \in \llbracket 1, n \rrbracket \mid b_i = 1 \right\}
\end{cases}$ est le rang de premier "pile" \\
(avec la convention $\min \emptyset = +\infty$)
\item $R: \begin{cases}
\Omega \to P(\llbracket 1, n \rrbracket) \\
(b_1, ...\,, b_n) \mapsto \left\{ i \in \llbracket 1, n \rrbracket \mid b_i = 1 \right\}
\end{cases}$ est l'ensemble des rangs où l'on a obtenu pile
\end{itemize} \medskip

\noindent \uline{Lien entre ces objets}:
\begin{itemize}
\item "Obtenir un nombre impair de 'pile' " et "Obtenir que 'face' " sont incompatibles: $I \cap F = \emptyset$
\item $F = \overline{\pi_1 \cup \pi_2 \cup ... \cup \pi_n} = \overline{\pi_1} \cap ... \cap \overline{\pi_n}$
\item Si $n = 3$, $I = (\pi_1 \cap \overline{\pi_2} \cap \overline{\pi_3}) \cup (\overline{\pi_1} \cap \pi_2 \cap \overline{\pi_3}) \cup (\overline{\pi_1} \cap \overline{\pi_2} \cap \pi_3) \cup (\pi_1 \cap \pi_2 \cap \pi_3)$
\item On a $N = L_1 + L_2 + ... + L_n$
\item On a $N = \left| R \right|$ \\
Formellement, $N = \card \circ R$, où $\card: \begin{cases}
P(\llbracket 1, n \rrbracket) \to \llbracket 0, n \rrbracket \\
T \mapsto \left| T \right|
\end{cases}$ \\
On a en fait utilisé la notation $f(x)$ des VA images (càd qu'on a noté $\card(R)$ plutôt que $\card \circ R$)
\item On a $L_1 + L_2 \leq N$ (en supposant $n \geq 2$)
\item $P = \min(R)$
\item $(N = 0) = F$
\item $(N \equiv 1 (\text{mod }2)) = (N \text{ impair}) = I$
\item $\pi_1 \cap ... \cap \pi_n = (N = n)$
\item $(L_i = 1) = \pi_i$: en fait, $L_i = \mathds{1}_{\pi_i}$
\item $\overline{\pi_1} = (p \geq 2)$
\item $(p = n) = \overline{\pi_1} \cap \overline{\pi_2} \cap ... \cap \overline{\pi_{n - 1}} \cap \pi_n = (R = \{ n \}) = (N = 1, L_n = 1)$
\end{itemize}

\section{Espaces probabilisés finis}
\subsection{Généralités}
\begin{definition}
\hfill
\begin{itemize}
\item Une \uline{mesure de probabilités} sur un univers $\Omega$ est une application $P: \mathcal{P}(\Omega) \to [0, 1]$ telle que:
\begin{itemize}
\item $P(\Omega) = 1$
\item Pour tous $A, B \in \mathcal{P}(\Omega)$ disjoints, $P(A \sqcup B) = P(A) + P(B)$
\end{itemize}
\item On appelle \uline{espace probabilisé (fini)} tout couple $(\Omega, P)$, où $\Omega$ est un univers et $P$ une mesure de \\
probabilités du $\Omega$
\end{itemize}
\end{definition}
\begin{proposition}
Soit $(\Omega, P)$ un espace probabilisé fini (epf). \\
On a:
\begin{itemize}
\item $P(\emptyset) = 0$
\item \uline{Croissance}: pour tous $A, B \in \mathcal{P}(\Omega)$, $A \subseteq B \implies P(A) \leq P(B)$
\item $\forall A \in \mathcal{P}(\Omega)$, $P(\overline{A}) = 1 - P(A)$
\item Pour tous $A_1, ...\,, A_r \in \mathcal{P}(\Omega)$ disjoints, \[P\left( \bigsqcup\limits_{i = 1}^r A_i \right) = \sum\limits_{i = 1}^r P(A_i)\]
\item $\forall A, B \in \mathcal{P}(\Omega)$, $P(A \cup B) = P(A) + P(B) - P(A \cap B)$
\end{itemize}
\end{proposition}

\pagebreak

\subsection{Formule des probabilités globales}
\begin{definition}
Soit $(\Omega, P)$ un epf.
Un \uline{système complet d'événements} (scé) est une famille $(C_i)_{i = 1}^r$ qui forme un recouvrement disjoint de $\Omega$, càd telle que:
\begin{itemize}
\item Les $C_i$ sont (deux à deux) disjoints: $\forall i, j \in \llbracket 1, r \rrbracket$, $i \neq j \implies C_i \cap C_j = \emptyset$
\item $\bigcup\limits_{i = 1} C_i = \Omega$
\end{itemize}
\end{definition}
\begin{theorem}[Formule des probabilités totales]
Soit $(\Omega, P)$ un epf et $(C_i)_{i = 1}^r$ un scé. \\
Alors $\forall A \in \mathcal{P}(\Omega)$, $P(A) = \sum\limits_{i = 1}^r P(A \cap C_i)$
\end{theorem}

\subsection{Loi d'une VA}
\begin{definition}
Soit $(\Omega, P)$ un epf et $X: \Omega \to E$ une VA. \\
La loi de $X$ est la donnée pour tout $S \subseteq E$ de la probabilité $P(X \in S) = P( (X \in S) )$
\end{definition}
\begin{proposition}
Soit $(\Omega, P)$ un epf et $X: \Omega \to E$ une VA. \\
La loi de $X$ est déterminée par les probabilités $P(X = x)$, pour $x$ décrivant $\im X$ \\
Plus précisément, pour tout $S \subseteq E$
\[P(X \in S) = \sum\limits_{x \in S \, \cap \, \im X} P(X = x)\]
\end{proposition}
\begin{definition}
Soit $(\Omega, P)$ un epf et $E$ un ensemble fini non vide. \\
Une VA $X: \Omega \to E$ \uline{suit la loi uniforme sur $E$} si $\forall S \in \mathcal{P}(E)$, $P(X \in S) = \frac{\left| S \right|}{\left| E \right|}$ \\
On note alors $X \sim U(E)$
\end{definition}
\begin{definition}
Soit $(\Omega, P)$ un epf. \\
Une VA $X: \Omega \to \{ 0, 1 \}$ suit le \uline{loi de Bernoulli} de paramètre $p \in \llbracket 0, 1 \rrbracket$ si $P(X = 1) = p$ \\
On note alors $X \sim B(p)$
\end{definition}
\noindent \uline{Remarque importante}: Si $A \subseteq \Omega$ est un événement, alors $\mathds{1}_A \sim B(p)$, où $p = P(A)$

\subsection{Couples de VA}
\begin{definition}
Soit un epf et $X_1: \Omega \to E_1$ et $X_2: \Omega \to E_2$ deux VA. \\
Le \uline{loi conjointe} de $X_1$ et $X_2$ est la loi de la VA
\[ (X_1, X_2) : \begin{cases}
\Omega \to E_1 \times E_2 \\
\omega \mapsto \left(X_1(\omega), X_2(\omega) \right)
\end{cases} \]
Les lois de $X_1$ et $X_2$ sont appelées \uline{lois marginales} de loi conjointe.
\end{definition}
\begin{proposition}[Calcul des marginales]
Avec les notations de la définition:
\[ \forall x_1 \in E_1 ,\, P(X_1 = x_1) = \sum_{x_2 \in \, \im X_2} P(X_1 = x_1, X_2 = x_2) \]
\end{proposition}
\noindent \uline{Remarque important}: \\
La loi conjointe détermine les lois marginales, la réciproque est fausse: il y a plusieurs manières de \uline{coupler} des lois. \\
Notamment, pour tout $p \in \left[ 0, \frac{1}{2} \right]$ on a
\begin{center}
\begin{tabular}{M{3em} | M{3em} | M{3em}}
$k$ \textbackslash $\,\, l$ & $0$ & $1$ \\
\hline
$0$ & $p$ & $\frac{1}{2} - p$ \\
\hline
$1$ & $\frac{1}{2} - p$ & $p$
\end{tabular}
\end{center}
qui constitue un couplage de $B\left(\frac{1}{2} \right)$ avec elle-même. \\
\uline{Trois cas particuliers}: \medskip

$p = \frac{1}{2}$ \quad
\begin{tabular}{M{3em} | M{3em}}
$\frac{1}{2}$ & $0$ \\
\hline
$0$ & $\frac{1}{2}$ \\
\end{tabular} \\
On a $P(X_1 = X_2) = 1$ \\
$X_1$ et $X_2$ sont égales (presque sûrement). \medskip

$p = 0$ \quad
\begin{tabular}{M{3em} | M{3em}}
$0$ & $\frac{1}{2}$ \\
\hline
$\frac{1}{2}$ & $0$ \\
\end{tabular} \\
$X_1 + X_2 = 1$ presque sûrement. \medskip

$p = \frac{1}{4}$ \quad
\begin{tabular}{M{3em} | M{3em}}
$\frac{1}{4}$ & $\frac{1}{4}$ \\
\hline
$\frac{1}{4}$ & $\frac{1}{4}$ \\
\end{tabular} \\
$P(X_1 = X_2) = \frac{1}{2}$ \\
Connaître le résultat de $X_1$ ne donne aucune information sur celui de $X_2$ \\
On dira que $X_1$ et $X_2$ sont indépendantes.

\subsection{Construction d'espaces probabilisés finis}
\begin{definition}
Une \uline{distribution de probabilités} sur un univers fini $\Omega$ est une famille $(p_\omega)_{\omega \in \Omega}$ de réels $\geq 0$ tels que $\sum\limits_{\omega \in \Omega} p_\omega = 1$
\end{definition}
\begin{proposition}
Soit $(p_\omega)_{\omega \in \Omega}$ une distribution de probabilités sur un univers fini $\Omega$ \\
Alors il existe une unique mesure de probabilités $P$ sur $\Omega$ telle que $\forall \omega \in \Omega$, $P(\{ \omega \}) = p_\omega$
\end{proposition}

\subsection{Modélisation(s) d'expériences aléatoires}
\noindent Imaginons qu'on veuille modéliser l'expérience constituant à tirer aléatoirement une carte dans un jeu de $52$ cartes. \medskip

\noindent \uline{Modélisation 1 (normale)}: On prend $\Omega = \left\{ 2, 3, ...\,, V, D, R, A \right\} \times \left\{ P, C, K, T \right\}$ l'ensemble des 52 cartes, muni de la probabilité uniforme. \\
L'événement (au sens usuel) "tirer un coeur" correspond à l'événement $\heartsuit = \left\{ (2, C), ...\,, (A, C) \right\}$ \\
donc $P(\heartsuit) = \frac{\left| \heartsuit \right|}{\left| \Omega \right|} = \frac{13}{52} = \frac{1}{4}$ \medskip

\noindent \uline{Modélisation 2 (un peu tordue)}: On commence par mélanger le jeu. On va poser $J = \left\{ 2, ...\,, R, A \right\} \times \left\{ P, C, K, T \right\}$ puis on pose $\Omega$ l'ensemble des permutations de $J$ (càd une $52$-liste sans répétition de $J$), muni de la mesure de probabilités uniforme. \\
L'événement (au sens usuel) "tirer un coeur" correspond à l'événement \\
$\heartsuit' = \left\{ (C_1, C_2, ...\,, C_{52}) \in \Omega' \mid C_1 \in \left\{ (2, C), ...\,, (A, C) \right\} \right\}$ \\
On a $\left| \Omega' \right| = 52!$ Calculons $\left| \heartsuit' \right|$ \\
Pour construire un élément de $\heartsuit'$:
\begin{itemize}
\item On choisit une première carte (un coeur): $13$ possibilités.
\item \rule{5em}{1pt} $2$è carte (différente de la $1$ère): $51$ possibilité.
\item \rule{5em}{1pt} $3$è carte (différente des précédentes): $50$ possibilités.
\end{itemize}
Par principe de multiplication, $\left| \heartsuit' \right| = 13 \times 51 \times 50 \times ... \times 1 = 13 \times 51!$ \\
Donc $P(\heartsuit') = \frac{13 \times 51!}{52!} = \frac{13}{52} = \frac{1}{4}$ \\
En théorie des possibilités, l'epf joue un rôle de seconde plan: dans l'exemple précédent, les détails de $\Omega$ n'importent pas. Ce qui compte est qu'il existe une VA donnant le résultat du tirage. Que l'on prenne
\[ X: \begin{cases}
\Omega \to J \\
C \mapsto C
\end{cases} \text{\quad ou \quad}
X': \begin{cases}
\Omega' \to J \\
(C_1, ...\,, C_{52} \mapsto C_1
\end{cases}\]
On a une VA $X$ ou $X' \sim U(J)$ et c'est ce qui compte.

\subsection{Vers les espaces probabilisés généraux}
\begin{definition}
Une \uline{tribu} (ou une \uline{$\sigma$-algèbre}) sur une ensemble $\Omega$ est une partie $a \in \mathcal{P}(\Omega)$ contenant $\Omega$ ($\Omega \in a$) stable par passage au complémentaire (si $A \in a$, $\overline{A} \in a$) et union dénombrable (si $A_0, A_1, ... \in a$, $\bigcup\limits_{n \in \mathbb{N}} A_n \in a$)
\end{definition}
\begin{definition}
Un espace probabilisé est un triplet ($\Omega$, $a$, $P$) où:
\begin{itemize}
\item \uline{L'univers} $\Omega$ est un ensemble non vide.
\item $a$ est une tribu sur $\Omega$
\item \uline{La mesure de probabilités} $P: a \to [0, 1]$ vérifie:
\begin{itemize}
\item $P(\Omega) = 1$
\item Si $(A_n)_{n \in \mathbb{N}}$ est une suite d'éléments de $a$, deux à deux disjoints, alors $P\left( \bigsqcup\limits_{n \in \mathbb{N}} A_n \right) = \sum\limits_{n \in \mathbb{N}} P(A_n)$ ($\sigma$-additivité)
\end{itemize}
\end{itemize}
\end{definition}
\begin{theorem}[culturel]
Il existe un espace probabilisé $([0, 1], B, \lambda)$ où:
\begin{itemize}
\item $B$ est une tribu sur $[0, 1]$ (tribu des boréliens) contenant les intervalles.
\item $\lambda: B \to [0, 1]$ est une mesure de probabilités telle que $\forall 0 \leq a < b \leq 1$, $\lambda(\left] a, b \right[) = b - a$ \\
appelée \uline{mesure de Lebesgue}.
\end{itemize}
\end{theorem}

\section{Indépendance}
Dans cette section, $(\Omega, P)$ est un épf.

\subsection{Deux événements}
\begin{definition}
Soit $A, B \in \mathcal{P}(\Omega)$ deux événements. \\
On dit que $A$ et $B$ sont \uline{indépendants} si $P(A \cap B) = P(A) P(B)$
\end{definition}
\begin{proposition}
Soit $A, B \in \mathcal{P}(\Omega)$ indépendants. \\
Alors $\overline{A}$ et $B$ sont indépendants.
\end{proposition}

\subsection{Variables aléatoires}
\begin{definition}
Soit $X_1: \Omega \to E_1$ et $X_2: \Omega \to E_2$ \\
On dit que $X_1$ et $X_2$ sont \uline{indépendantes} (et on note $X_1 \indep X_2$) si $\forall S_1 \in \mathcal{P}(E_1)$, $\forall S_2 \in \mathcal{P}(E_2)$
\[ P(X_1 \in S_1, X_2 \in S_2) = P(X_1 \in S_1) P(X_2 \in S_2) \]
Plus généralement, soit $X_i: \Omega \to E_i$ des VA ($i \in \llbracket 1, n \rrbracket$). \\
On dit qu'elles sont \uline{indépendantes} si $\forall S_1 \in \mathcal{P}(E_1), ...\,, \forall S_n \in \mathcal{P}(E_n)$
\[ P(X_1 \in S_1, ...\,, X_n \in S_n) = P(X_1 \in S_1) ... P(X_n \in S_n) \]
\end{definition}
\noindent \uline{Remarque}: Si $n$ VA sont indépendantes (ou, pour insister \uline{mutuellement} indépendantes), elles sont \\
\uline{indépendantes deux à deux}, càd $\forall i \neq j \in \llbracket 1, n \rrbracket$, $X_i \indep X_j$
\begin{theorem}
Soit $X_1: \Omega \to E_1, ...\,, X_n: \Omega \to E_n$ des VA. \\
Alors $X_1, ...\,, X_n$ sont indépendantes ssi
\[ \forall x_1 \in E_1, ...\,, \forall x_n \in E_n,\, P(X_1 = x_1, ...\,, X_n = x_n) = P(X_1 = x_1) ... P(X_n = x_n) \]
\end{theorem}
\begin{proposition}
Soit $A, B \in \mathcal{P}(\Omega)$ \\
Alors $A$ et $B$ sont indépendants ssi $\mathds{1}_A \indep \mathds{1}_B$
\end{proposition}

\subsection{Plusieurs événements}
\begin{definition}
Soit $A_1, ...\,, A_n \in \mathcal{P}(\Omega)$ \\
Alors ils sont \uline{indépendants} si $\mathds{1}_{A_1}, ...\,, \mathds{1}_{A_n}$ sont indépendants.
\end{definition}
\begin{theorem}
Soit $A_1, ...\,, A_n \in \mathcal{P}(\Omega)$ \\
Alors $A_1, ...\,, A_n$ sont indépendants ssi
\[\forall I \subseteq \llbracket 1, n \rrbracket,\, P\left( \bigcap\limits_{i \in I} A_i \right) = \prod\limits_{i \in I} P(A_i)\]
\end{theorem}

\subsection{Stabilité de l'indépendance}
\begin{theorem}[Transfert de l'indépendance]
\hfill \\
Soit $X_1: \Omega \to E_1, ...\,, X_n: \Omega \to E_n$ des VA indépendantes. \\
Soit $f_1: E_1 \to F_1, ...\,, f_n: E_n \to F_n$ des applications. \\
Alors $f_1(X_1), ...\,, f_n(X_n)$ sont indépendantes.
\end{theorem}
\begin{corollaire}
Si $X_1: \Omega \to E_1, ...\,, X_n: \Omega \to E_n$ sont indépendantes et que, pour tout $i \in \llbracket 1, n \rrbracket$, $A_i$ est un événement défini par $X_i$, alors $A_1, ...\,, A_n$ sont indépendants.
\end{corollaire}
\begin{theorem}[Lemme de coalitions]
Soit $X_1: \Omega \to E_1, ...\,, X_n: \Omega \to E_n$ indépendantes et $r \in \llbracket 0, n \rrbracket$ \\
Alors les VA
\begin{align*}&Y: \begin{cases}
\Omega \to E_1 \times ... \times E_r \\
\omega \mapsto (X_1(\omega), ...\,, X_r(\omega))
\end{cases} 
&Z: \begin{cases}
\Omega \to E_{r + 1} \times ... \times E_n \\
\omega \mapsto (X_{r + 1}(\omega), ...\,, X_n(\omega))
\end{cases}
\end{align*}
sont indépendantes.
\end{theorem}

\subsection{Indépendance et mesure uniforme}
\begin{theorem}
Soit $E_1, ...\,, E_n$ des ensembles finis non vides et on munit $\Omega = E_1 \times ... \times E_n$ de la probabilité uniforme. \\
Alors
\[ X_1: \begin{cases}
\Omega \to E_1 \\
(x_1, ...\,, x_n) \mapsto x_1
\end{cases}
, ...\,, \quad
X_n: \begin{cases}
\Omega \to E_n \\
(x_1, ...\,, x_n) \mapsto x_n
\end{cases} \]
sont indépendantes.
\end{theorem}

\subsection{Loi binomiale}
\begin{definition}
Une VA $X: \Omega \to \llbracket 0, n \rrbracket$ \uline{suit la loi binomiale} de paramètre $n$ et $p \in [ 0, 1 ]$ ($X \sim B(n, p)$) \\
si $\forall k \in \llbracket 0, n \rrbracket$
\[P(X = k) = \binom{n}{k} p^k (1 - p)^{n - k}\]
\end{definition}
\begin{theorem}
Soit $X_1, ...\,, X_n: \Omega \to \{ 0, 1 \}$ indépendantes suivant la loi de Bernoulli $B(p)$ (pour $p \in [0, 1]$) \\
Alors $X_1 + ... + X_n \sim B(n, p)$
\end{theorem}

\pagebreak

\section{Probabilités conditionnelles}
On se place dans un espace probabilisé fini $(\Omega, P)$

\subsection{Définition}
\begin{definition}
Soit $B \in \mathcal{P}(\Omega)$ un événement non négligeable (càd $P(B) > 0$). Soit $A \in \mathcal{P}(\Omega)$ \\
On définit la probabilité conditionnelle de $A$ sachant $B$
\[ P_B(A) = P(A \mid B) = \frac{P(A \cap B)}{P(B)} \]
\end{definition}
\begin{proposition}
Soit $B \in \mathcal{P}(\Omega)$ tel que $P(B) > 0$ \\
Alors $P_B$ est une mesure de probabilités sur $\Omega$ pour laquelle $B$ est presque sûr.
\end{proposition}
\noindent \uline{Attention!} La notation $(A \mid B)$ seule n'a aucun sens. \\
Par exemple, la phrase française " Sachant que $X = Y$, '$X = 0$' équivaut à '$Y = 0$' " ne se traduit pas en $(X = 0 \mid X = Y) = (Y = 0)$ qui n'a \uline{aucun} sens. \\
Elle ne se traduit pas non plus en $P(X = 0 \mid X = Y) = P(Y = 0)$ (qui a un sens mais qui est faux en général). \\
La bonne traduction est $(X = Y, X = 0) = (X = Y, Y = 0)$
\begin{proposition}
Soit $A, B \in \mathcal{P}(\Omega)$ tels que $P(B) > 0$ \\
Alors $A$ et $B$ sont indépendants ssi $P(A \mid B) = P(A)$
\end{proposition}
\begin{definition}
Soit $X: \Omega \to E$ une VA et $A \in \mathcal{P}(\Omega)$ un événement non négligeable. \\
La \uline{loi conditionnelle de $X$ sachant $A$} est la donnée, pour tout $S \subseteq E$ de $P(X \in S \mid A)$
\end{definition}

\subsection{Probabilités composées et probabilités totales}
\begin{proposition}[Formule des probabilités composées]
\hfill \\
Soit $A_1, ...\,, A_n \in \mathcal{P}(\Omega)$ tels que $\bigcap\limits_{i = 1}^{n - 1} A_i$ soit non négligeable. \\
On a 
\[ P\left( \bigcap\limits_{i = 1}^n A_i \right) = P(A_1) P(A_2 \mid A_1) P(A_3 \mid A_2 \cap A_1) ... P\left( A_n \mid \bigcap\limits_{i = 1}^{n - 1} A_i \right) \]
\end{proposition}
\noindent \uline{Remarque}: Si $\bigcap\limits_{i = 1}^{n - 1} A_i$ est négligeable, la formule reste correcte si on pose la convention $0 \times $ inepte $= 0$
\begin{proposition}[Formule des probabilités totales]
Soit $A \in \mathcal{P}(\Omega)$ et $C_1, ...\,, C_n \in \mathcal{P}(\Omega)$ des événements \\
(non négligeables) formant un système complet d'événements. \\
Alors 
\[ P(A) = \sum_{i = 1}^n P(A \cap C_i) = \sum_{i = 1}^n P(A \mid C_i) P(C_i) \]
\end{proposition} 
\begin{corollaire}
Soit $A, B \in \mathcal{P}(\Omega)$ (tels que $P(B) \in \left] 0, 1 \right[$) \\
Alors $P(A) = P(A \mid B) P(B) + P(A \mid \overline{B}) P(\overline{B})$
\end{corollaire}

\subsection{Formules de Bayes}
\begin{proposition}
Soit $A, B$ deux événements non négligeables. \\
On a
\[P(B \mid A) = \frac{P(A \mid B) P(B)}{P(A)}\]
\end{proposition}
\begin{proposition}
Soit $A$ un événement et $(C_i)_{i = 1}^n$ un scé, tous non négligeables. \\
Alors, pour tout $i \in \llbracket 1, n \rrbracket$
\[P(C_i \mid A) = \frac{P(A \mid C_i)P(C_i)}{P(A)} = \frac{P(A \mid C_i)P(C_i)}{\sum\limits_{j = 1}^n P(A \mid C_j) P(C_j)} \]
\end{proposition}

\section{Espérance}
Dans toute cette section, $X$ sera une variable aléatoire à valeurs réelle ou complexe, voire à valeurs dans un espace vectoriel réel.
\begin{definition}
Soit $X$ une VA complexe. \\
On définit son \uline{espérance}: 
\[E(X) = \sum\limits_{x \in \im(X)} P(X = x)x\]
\end{definition}
\noindent \uline{Remarque}: $E(X)$ ne dépend que de la loi de $X$: \\
Si $X'$ est une VA de même loi que $X$ (ce que l'on note $X \sim X'$), alors $E(X) = E(X')$
\begin{theorem}
L'espérance possède les propriétés suivantes: \\
\uline{Linéarité}: L'espérance est une forme linéaire $\mathbb{C}^\Omega \to \mathbb{C}$ (ou $\mathbb{R}^\Omega \to \mathbb{R}$) \\
Concrètement, si $X, Y$ sont deux VA complexes et $\lambda \in \mathbb{C}$, alors $E(X + \lambda Y) = E(X) + \lambda E(Y)$ \\
\uline{Positivité}: Si $X: \Omega \to \mathbb{R}_+$ est une VA positive, on a $E(X) \geq 0$ \\
\uline{Croissante}: Si $X, Y: \Omega \to \mathbb{R}$ sont deux VA telles que $X \leq Y$ (càd $\forall \omega \in \Omega$, $X(\omega) \leq Y(\omega)$), alors $E(X) \leq E(Y)$ \\
\uline{Inégalité triangulaire}: Si $X: \Omega \to \mathbb{C}$ est une VA complexe, $|E(X)| \leq E(|X|)$
\end{theorem}
\subsection{Définition}
\begin{lemme}
Si $X$ est une VA complexe, $E(X) = \sum\limits_{\omega \in \Omega} P(\{\omega\})X(\omega)$
\end{lemme}
\begin{proposition}
Soit $X \sim B(n, p)$ \\
Alors $E(X) = np$
\end{proposition}
\begin{theorem}[Formule de transfert]
Soit $X: \Omega \to E$ et $f:E \to \mathbb{C}$ une application. \\
Alors
\[E(f(x)) = \sum\limits_{x \in \im(X)}P(X = x)f(x)\]
\end{theorem}
\begin{corollaire}[Formule de transfert bivariée]
Soit $X: \Omega \to E$ et $Y: \Omega \to F$ deux VA et $f: E \times F \to \mathbb{C}$ \\
Alors
\[E(f(X, Y)) = \sum\limits_{\substack{x \in \im X \\ y \in \im Y}} P(X = x, Y = y)f(x, y) \]
\end{corollaire}
\begin{definition}
Une VA $X: \Omega \to \mathbb{C}$ est \uline{centrée} si $E(X) = 0$
\end{definition}

\subsection{Applications}
\begin{theorem}[Formule du crible ou d'inclusion-exclusion]
Soit $A_1, ...\,, A_n \in \mathcal{P}(\Omega)$ \\
On a
\[ P\left(\bigcup_{i = 1}^n A_i \right) = \sum_{k = 1}^n (-1)^{k - 1} \sum_{1 \leq i_1 < ... < i_k \leq n} P\left(A_{i_1} \cap ... \cap A_{i_k}\right) \]
\end{theorem}
\begin{corollaire}
Soit $\Omega$ un ensemble non vide et $A_1, ...\,, A_n \in \mathcal{P}(\Omega)$ \\
Alors 
\[ \left| \bigcup_{i = 1}^n A_i \right| = \sum_{k = 1}^n (-1)^{k - 1} \sum_{1 \leq i_1 < ... < i_k \leq n} \left| A_{i_1} \cap ... \cap A_{i_k} \right| \]
\end{corollaire}

\subsection{Indépendance}
\begin{theorem}
Soit $X, Y: \Omega \to \mathbb{C}$ deux VA indépendantes. \\
Alors $E(XY) = E(X) E(Y)$ \\
Évidemment, cela se généralise à $n$ VA.
\end{theorem}

\subsection{Espérance conditionnelle}
\begin{definition}
Soit $X: \Omega \to \mathbb{C}$ une VA et $B \in \mathcal{P}(\Omega)$ non négligeable. \\
On définit l'espérance conditionnelle de $X$ sachant $B$:
\[ E(X \mid B) = \sum_{i \in \im X} P(X = x \mid B) x \]
\end{definition}
\begin{theorem}[Formule des espérances totales]
Soit $X: \Omega \to \mathbb{C}$ une VA et $(C_i)_{i = 1}^n$ un scé non négligeables. \\
Alors
\[ E(X) = \sum_{i = 1}^n E(X \mid C_i)P(C_i) \]
\end{theorem}

\section{Moments d'ordre deux}
\noindent \uline{Remarque}: Pour $k \in \mathbb{N}$, l'espérance $E(X^k)$ s'appelle le \uline{$k$-ième moment} de $X$
\subsection{Variance et écart-type}
\begin{definition}
Soit $X: \Omega \to \mathbb{R}$ une VA réelle. \\
On définit sa \uline{variance}:
\[ V(X) = E((X - E(X))^2) \]
On définit \uline{l'écart-type}:
\[ \sigma(X) = \sqrt{V(X)} \]
\end{definition}
\begin{proposition}[Formule de König-Huygens]
Soit $X: \Omega \to \mathbb{R}$
Alors
\[ V(X) = E(X^2) - E(X)^2 \]
\end{proposition}
\noindent \uline{Exemple important}: Si $X \sim B(p)$, on a $E(X) = p$ \\
Donc $V(X) = p(1 - p)$
\begin{proposition}
Soit $X: \Omega \to \mathbb{R}$ et $a, b \in \mathbb{R}$ \\
On a $V(aX + b) = a^2 V(X)$
\end{proposition}
\begin{proposition}
Soit $X: \Omega \to \mathbb{R}$ \\
On a $V(X) = 0 \implies X$ est constante presque sûrement. \\
(et signifie qu'il existe $c \in \mathbb{R}$ tel que $P(X = c) = 1$
\end{proposition}

\subsection{Covariance}
\begin{definition}
Soit $X, Y: \Omega \to \mathbb{R}$ \\
On définit la \uline{covariance de $X$ et $Y$}: 
\[ \cov(X, Y) = E\left((X - EX)(Y - EY)\right) \]
\end{definition}
\begin{proposition}
On a
\[ \cov(X, Y) = E(XY) - EX \cdot EY \]
\end{proposition}
\noindent \uline{Remarque}: Si $X \indep Y$, on a donc $\cov(X, Y) = 0$. On dit que $X$ et $Y$ sont \uline{décorrélées}. \\
Si $\cov(X, Y) > 0$ (resp. $< 0$), on dit que $X$ et $Y$ sont \uline{positivement} (resp. \uline{négativement}) corrélées. \medskip

\noindent \uline{Remarque}: La covariance est \uline{presque} un produit scalaire (il manque le caractère défini) \\
On garde donc toutes les propriétés liées à la bilinéarité et au caractère positif, notamment:
\begin{itemize}
\item L'identité remarquable $V(X + Y) = V(X) + 2\cov(X, Y) + V(Y)$
\item L'inégalité de Cauchy-Schwarz (sans le cas d'égalité)
\end{itemize}
\begin{theorem}
Soit $X, Y: \Omega \to \mathbb{R}$ \\
On a
\[ \cov(X, Y) \leq |\cov(X, Y)| \leq \sqrt{V(X)} \sqrt{V(Y)} \]
\end{theorem}
\begin{theorem}
Soit $X_1, ...\,, X_n: \Omega \to \mathbb{R}$ deux à deux décorrélées. \\
Alors 
\[ V(X_1 + ... + X_n) = V(X_1) + ... + V(X_n) \]
\end{theorem}
\begin{corollaire}
Soit $X \sim B(n, p)$ \\
Alors $V(X) = np(1-p)$
\end{corollaire}

\section{Inégalités de concentration}
\subsection{Inégalité de Markov}
\begin{theorem}
Soit $X: \Omega \to \mathbb{R}_+$ (\uline{à valeurs positives}) \\
Alors $\forall a > 0$
\[ P(X \geq a) \leq \frac{E(X)}{a} \]
\end{theorem}

\subsection{Inégalité de Bienaymé-Tchebychev}
\begin{theorem}
Soit $X: \Omega \to \mathbb{R}$ \\
Alors $\forall a > 0$
\[ P(|X - EX| \geq a) \leq \frac{V(X)}{a^2} \]
\end{theorem}

\subsection{Loi faible des grands nombres}
\begin{theorem}
Soit $X_1, ...\,, X_n: \Omega_n \to \mathbb{R}$ indépendantes et de même loi. \\
Notons $\mu$ leur espérance. \\
On définit 
\[S_n = \frac{X_1 + ... + X_n}{n}\]
Alors $\forall \varepsilon > 0$
\[ P(|S_n - \mu| \geq \varepsilon) \xrightarrow[n \to +\infty]{} 0 \]
\end{theorem}

\subsection{Théorème d'approximation de Weierstrass}
\begin{theorem}
Soit $f \in C^0([0, 1])$ \\
Alors il existe une suite de fonctions polynomiales $(P_n)_{n \in \mathbb{N}^*}$ qui converge uniformément vers $f$
\end{theorem}
\end{document}